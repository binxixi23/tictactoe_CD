class QLearningAgent:
    def __init__(self, learning_rate=0.8, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.q_table = {}
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

    def get_action(self, state, available_actions):
        if state not in self.q_table:
            self.q_table[state] = {action: 0.0 for action in available_actions}

        if random.random() < self.epsilon:
            return random.choice(available_actions)
        else:
            return max(self.q_table[state].items(), key=lambda x: x[1])[0]
    def choose_action(self, state, available_moves):
        if random.random() < self.epsilon:
            return random.choice(available_moves)
        q_values = [self.get_q_value(state, action) for action in available_moves]
        max_q = max(q_values)
        max_actions = [action for action, q in zip(available_moves, q_values) if q == max_q]
        return random.choice(max_actions)      
    
    def get_q_value(self, state, action):
        return self.q_table.get((state, action), 0.0)
    def update_q_table(self, state, action, reward, next_state, next_available_moves):
        current_q = self.get_q_value(state, action)
        next_max_q = max([self.get_q_value(next_state, next_action) for next_action in next_available_moves], default=0)
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * next_max_q - current_q)
        self.q_table[(state, action)] = new_q


    def update(self, state, action, reward, next_state, next_available_actions, done):
        if state not in self.q_table:
            self.q_table[state] = {action: 0.0 for action in self.available_actions(state)}
        if next_state not in self.q_table and not done:
            self.q_table[next_state] = {action: 0.0 for action in next_available_actions}

        current_q = self.q_table[state][action]
        next_max_q = 0 if done else max(self.q_table[next_state].values())
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * next_max_q - current_q)
        self.q_table[state][action] = new_q

    def decay_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
    def update_q_value(self, state, action, reward, next_state, next_available_actions):
        current_q = self.get_q_value(state, action)
        next_q_values = [self.get_q_value(next_state, a) for a in next_available_actions]
        max_next_q = max(next_q_values) if next_q_values else 0
        self.q_table[(state, action)] = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
  
